Import("env") # type: ignore
from pathlib import Path
import gzip
import hashlib
import re
import json

input_dir = env.GetProjectOption("custom_webfiles_input", "website") # type: ignore
output_file = env.GetProjectOption("custom_webfiles_output", "src/webfiles.h") # type: ignore
build_info_file = env.GetProjectOption("custom_webfiles_build_info_output", "src/build_info.h") # type: ignore
hash_algo = env.GetProjectOption("custom_webfiles_hash_algo", "sha256").lower() # type: ignore

PROJECT_DIR = Path(env["PROJECT_DIR"]) # type: ignore
input_path = PROJECT_DIR / input_dir
output_path = PROJECT_DIR / output_file
build_info_path = PROJECT_DIR / build_info_file

HASH_DEFINE = "WEBFILES_HASH"
HASH_REGEX = re.compile(rf'#define\s+{re.escape(HASH_DEFINE)}\s+"([^"]+)"')

BUILD_HASH_DEFINE = "ESPWEBUTILS_WEBFILES_HASH"
BUILD_HASH_REGEX = re.compile(rf'#define\s+{re.escape(BUILD_HASH_DEFINE)}\s+"([^"]+)"')

LIB_VERSION_DEFINE = "ESPWEBUTILS_LIBRARY_VERSION"

def read_library_version(project_dir: Path) -> str:
    """Read library version from library.json / libary.json.

    We support the misspelled libary.json because it's used in this repo.
    """
    for filename in ("library.json", "libary.json"):
        p = project_dir / filename
        if not p.exists():
            continue
        try:
            data = json.loads(p.read_text(encoding="utf-8"))
            v = data.get("version")
            if isinstance(v, str) and v.strip():
                return v.strip()
        except Exception:
            pass
    return "unknown"

def to_c_array(data: bytes) -> str:
    return ", ".join(f"0x{b:02x}" for b in data)

def sanitize_filename(path: Path) -> str:
    return str(path.as_posix()).replace("/", "_").replace(".", "_").replace("-", "_")

def add_version_to_assets(content: str, version: str) -> str:
    """Add version query parameter to CSS/JS/font URLs in HTML to enable cache busting."""
    # Match href="/css/*.css" and add ?v=version if not already present
    content = re.sub(
        r'href=(["\'])(/(?:css|fonts)/[^"\']+\.(?:css|woff2|woff|ttf))\1',
        lambda m: f'href={m.group(1)}{m.group(2)}?v={version}{m.group(1)}' if '?' not in m.group(2) else m.group(0),
        content
    )
    # Match src="/js/*.js" and add ?v=version if not already present
    content = re.sub(
        r'src=(["\'])(/js/[^"\']+\.js)\1',
        lambda m: f'src={m.group(1)}{m.group(2)}?v={version}{m.group(1)}' if '?' not in m.group(2) else m.group(0),
        content
    )
    return content

def list_files(in_dir: Path):
    return sorted([p for p in in_dir.rglob("*") if p.is_file()])

def compute_tree_hash(in_dir: Path, algo: str, lib_version: str = "", extra: bytes = b"") -> str:
    try:
        h = hashlib.new(algo)
    except ValueError:
        print(f"[webfiles] ERROR: unknown hash algo '{algo}'. Use sha256/sha1/md5/...")
        Exit(1) # type: ignore

    files = list_files(in_dir)

    for f in files:
        rel = f.relative_to(in_dir).as_posix().encode("utf-8")
        h.update(rel)
        h.update(b"\x00")
        
        file_content = f.read_bytes()
        
        # For HTML files, apply version modifications before hashing
        if f.suffix == '.html' and lib_version:
            try:
                html_str = file_content.decode('utf-8')
                html_str = add_version_to_assets(html_str, lib_version)
                file_content = html_str.encode('utf-8')
            except Exception:
                pass  # If conversion fails, use original content
        
        h.update(file_content)
        h.update(b"\x00")

    if extra:
        h.update(b"__EXTRA__")
        h.update(extra)
        h.update(b"\x00")

    return h.hexdigest()

def read_existing_hash(out_file: Path):
    if not out_file.exists():
        return None
    try:
        text = out_file.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return None

    m = BUILD_HASH_REGEX.search(text)
    if m:
        return m.group(1)

    # Backwards compat if file was the old webfiles.h
    m = HASH_REGEX.search(text)
    return m.group(1) if m else None

def generate_build_info_header(out_file: Path, tree_hash: str, lib_version: str) -> None:
    out_file.parent.mkdir(parents=True, exist_ok=True)
    with out_file.open("w", encoding="utf-8") as out:
        out.write("#pragma once\n\n")
        out.write("// Auto-generated by PlatformIO extra_script: generate_webfiles.py\n")
        out.write(f'#define {LIB_VERSION_DEFINE} "{lib_version}"\n')
        out.write(f'#define {BUILD_HASH_DEFINE} "{tree_hash}"\n')
        # Keep the old macro name available for existing code.
        out.write(f'#define {HASH_DEFINE} {BUILD_HASH_DEFINE}\n')

def generate_header(in_dir: Path, out_file: Path, lib_version: str) -> int:
    files = list_files(in_dir)
    out_file.parent.mkdir(parents=True, exist_ok=True)

    file_map_entries = []

    with out_file.open("w", encoding="utf-8") as out:
        out.write("#pragma once\n\n")
        out.write('#include "build_info.h"\n\n')
        out.write("#include <pgmspace.h>\n")
        out.write("#include <cstdint>\n")
        out.write("#include <cstddef>\n\n")
        out.write("// Auto-generated by PlatformIO extra_script: generate_webfiles.py\n")
        out.write("\n")

        for file in files:
            rel_path = file.relative_to(in_dir)
            var_name = sanitize_filename(rel_path)

            original_data = file.read_bytes()
            
            # For HTML files, add version to asset URLs for cache busting
            if rel_path.suffix == '.html':
                try:
                    html_str = original_data.decode('utf-8')
                    html_str = add_version_to_assets(html_str, lib_version)
                    original_data = html_str.encode('utf-8')
                except Exception as e:
                    print(f"[webfiles] WARNING: Could not process HTML file {rel_path}: {e}")
            
            compressed_data = gzip.compress(original_data)

            out.write(f"// {rel_path.as_posix()} ({len(original_data)} bytes, gzipped to {len(compressed_data)} bytes)\n")
            out.write(f"const uint8_t {var_name}[] PROGMEM = {{\n  {to_c_array(compressed_data)}\n}};\n\n")

            file_map_entries.append((f'/{rel_path.as_posix()}', var_name, len(compressed_data)))

        out.write("struct WebFile {\n  const char* path;\n  const uint8_t* data;\n  size_t size;\n};\n\n")

        out.write("const WebFile webFiles[] = {\n")
        for path, var, size in file_map_entries:
            out.write(f'  {{ "{path}", {var}, {size} }},\n')
        out.write("};\n\n")

        out.write(f"const size_t webFilesCount = {len(file_map_entries)};\n")

    return len(file_map_entries)

if not input_path.exists():
    print(f"[webfiles] WARN: input dir not found: {input_path} (skip)")
else:
    lib_version = read_library_version(PROJECT_DIR)
    current_hash = compute_tree_hash(input_path, hash_algo, lib_version=lib_version, extra=f"lib_version={lib_version}".encode("utf-8"))
    old_hash = read_existing_hash(build_info_path)

    if old_hash == current_hash:
        print(f"[webfiles] unchanged (hash={current_hash[:12]}...), skip: {output_path}")
    else:
        generate_build_info_header(build_info_path, current_hash, lib_version)
        count = generate_header(input_path, output_path, lib_version)
        if old_hash:
            print(f"[webfiles] changed: {old_hash[:12]}... -> {current_hash[:12]}...")
        else:
            print(f"[webfiles] first gen: {current_hash[:12]}...")
        print(f"[webfiles] generated {output_path} ({count} files) from {input_path}")
        print(f"[webfiles] generated {build_info_path}")
