Import("env") # type: ignore
from pathlib import Path
import gzip
import hashlib
import re

input_dir = env.GetProjectOption("custom_webfiles_input", "website") # type: ignore
output_file = env.GetProjectOption("custom_webfiles_output", "src/webfiles.h") # type: ignore
hash_algo = env.GetProjectOption("custom_webfiles_hash_algo", "sha256").lower() # type: ignore

PROJECT_DIR = Path(env["PROJECT_DIR"]) # type: ignore
input_path = PROJECT_DIR / input_dir
output_path = PROJECT_DIR / output_file

HASH_DEFINE = "WEBFILES_HASH"
HASH_REGEX = re.compile(rf'#define\s+{re.escape(HASH_DEFINE)}\s+"([^"]+)"')

def to_c_array(data: bytes) -> str:
    return ", ".join(f"0x{b:02x}" for b in data)

def sanitize_filename(path: Path) -> str:
    return str(path.as_posix()).replace("/", "_").replace(".", "_").replace("-", "_")

def list_files(in_dir: Path):
    return sorted([p for p in in_dir.rglob("*") if p.is_file()])

def compute_tree_hash(in_dir: Path, algo: str) -> str:
    try:
        h = hashlib.new(algo)
    except ValueError:
        print(f"[webfiles] ERROR: unknown hash algo '{algo}'. Use sha256/sha1/md5/...")
        Exit(1) # type: ignore

    files = list_files(in_dir)

    for f in files:
        rel = f.relative_to(in_dir).as_posix().encode("utf-8")
        h.update(rel)
        h.update(b"\x00")
        h.update(f.read_bytes())
        h.update(b"\x00")

    return h.hexdigest()

def read_existing_hash(out_file: Path):
    if not out_file.exists():
        return None
    try:
        text = out_file.read_text(encoding="utf-8", errors="ignore")
    except Exception:
        return None

    m = HASH_REGEX.search(text)
    return m.group(1) if m else None

def generate_header(in_dir: Path, out_file: Path, tree_hash: str) -> int:
    files = list_files(in_dir)
    out_file.parent.mkdir(parents=True, exist_ok=True)

    file_map_entries = []

    with out_file.open("w", encoding="utf-8") as out:
        out.write("#pragma once\n\n")
        out.write("#include <pgmspace.h>\n")
        out.write("#include <cstdint>\n")
        out.write("#include <cstddef>\n\n")
        out.write("// Auto-generated by PlatformIO extra_script: generate_webfiles.py\n")
        out.write(f'#define {HASH_DEFINE} "{tree_hash}"\n\n')

        for file in files:
            rel_path = file.relative_to(in_dir)
            var_name = sanitize_filename(rel_path)

            original_data = file.read_bytes()
            compressed_data = gzip.compress(original_data)

            out.write(f"// {rel_path.as_posix()} ({len(original_data)} bytes, gzipped to {len(compressed_data)} bytes)\n")
            out.write(f"const uint8_t {var_name}[] PROGMEM = {{\n  {to_c_array(compressed_data)}\n}};\n\n")

            file_map_entries.append((f'/{rel_path.as_posix()}', var_name, len(compressed_data)))

        out.write("struct WebFile {\n  const char* path;\n  const uint8_t* data;\n  size_t size;\n};\n\n")

        out.write("const WebFile webFiles[] = {\n")
        for path, var, size in file_map_entries:
            out.write(f'  {{ "{path}", {var}, {size} }},\n')
        out.write("};\n\n")

        out.write(f"const size_t webFilesCount = {len(file_map_entries)};\n")

    return len(file_map_entries)

if not input_path.exists():
    print(f"[webfiles] WARN: input dir not found: {input_path} (skip)")
else:
    current_hash = compute_tree_hash(input_path, hash_algo)
    old_hash = read_existing_hash(output_path)

    if old_hash == current_hash:
        print(f"[webfiles] unchanged (hash={current_hash[:12]}...), skip: {output_path}")
    else:
        count = generate_header(input_path, output_path, current_hash)
        if old_hash:
            print(f"[webfiles] changed: {old_hash[:12]}... -> {current_hash[:12]}...")
        else:
            print(f"[webfiles] first gen: {current_hash[:12]}...")
        print(f"[webfiles] generated {output_path} ({count} files) from {input_path}")
